#!/usr/bin/env python3
"""
vFlowå·¥ä½œæµä»“åº“ç´¢å¼•ç”Ÿæˆå™¨
è‡ªåŠ¨æ‰«æworkflowsç›®å½•å¹¶ç”Ÿæˆindex.json
"""

import json
import os
import sys
from datetime import datetime
from pathlib import Path


def normalize_id(filename):
    """ä»Žæ–‡ä»¶åèŽ·å–IDï¼ˆåŽ»é™¤.jsonæ‰©å±•åï¼‰"""
    return filename.replace('.json', '') if filename.endswith('.json') else filename


def validate_workflow(data, filename):
    """
    éªŒè¯å·¥ä½œæµæ•°æ®
    è¿”å›ž: (is_valid, error_message, cleaned_data)
    """
    # æ£€æŸ¥æ˜¯å¦æœ‰_meta
    if '_meta' not in data:
        return False, f"ç¼ºå°‘ '_meta' å­—æ®µ", None

    meta = data['_meta']

    # éªŒè¯_metaå¿…éœ€å­—æ®µ
    required_meta_fields = ['id', 'name', 'description', 'author', 'version', 'vFlowLevel']
    missing_fields = [field for field in required_meta_fields if field not in meta]

    if missing_fields:
        return False, f"_metaç¼ºå°‘å¿…éœ€å­—æ®µ: {', '.join(missing_fields)}", None

    # éªŒè¯_metaä¸­çš„IDä¸Žæ–‡ä»¶åä¸€è‡´
    expected_id = normalize_id(filename)
    meta_id = meta['id']

    if meta_id != expected_id:
        return False, f"_meta.id ä¸åŒ¹é…: æ–‡ä»¶å='{expected_id}', _meta.id='{meta_id}'", None

    # æ£€æŸ¥å·¥ä½œæµæ˜¯å¦æœ‰å…³é”®å­—æ®µï¼ˆå¯é€‰ï¼Œä¸ºäº†å…¼å®¹æ€§ï¼‰
    # if 'id' not in data or 'steps' not in data:
    #     return False, "å·¥ä½œæµç¼ºå°‘ 'id' æˆ– 'steps' å­—æ®µ", None

    return True, None, data


def clean_workflow_for_repo(data):
    """
    æ¸…ç†å·¥ä½œæµæ•°æ®ï¼Œå‡†å¤‡å‘å¸ƒåˆ°ä»“åº“
    - å°†isEnabledã€isFavoriteã€wasEnabledBeforePermissionsLostè®¾ç½®ä¸ºfalse
    - ä¿ç•™_metaä¿¡æ¯
    """
    cleaned = data.copy()

    # å¼ºåˆ¶è®¾ç½®ä¸ºfalseçš„å­—æ®µ
    cleaned['isEnabled'] = False
    cleaned['isFavorite'] = False
    cleaned['wasEnabledBeforePermissionsLost'] = False

    return cleaned


def scan_directory(directory_path):
    """
    æ‰«æç›®å½•ä¸­çš„æ‰€æœ‰å·¥ä½œæµJSONæ–‡ä»¶
    è¿”å›ž: (valid_items, errors, skipped_files)
    """
    items = []
    errors = []
    skipped_files = []

    dir_path = Path(directory_path)

    if not dir_path.exists():
        print(f"âŒ é”™è¯¯: ç›®å½•ä¸å­˜åœ¨: {directory_path}")
        return items, errors, skipped_files

    # éåŽ†ç›®å½•ä¸­çš„æ‰€æœ‰JSONæ–‡ä»¶
    for filepath in dir_path.glob('*.json'):
        # è·³è¿‡index.json
        if filepath.name == 'index.json':
            continue

        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # éªŒè¯å·¥ä½œæµ
            is_valid, error_msg, _ = validate_workflow(data, filepath.name)

            if not is_valid:
                errors.append(f"âŒ {filepath.name}: {error_msg}")
                skipped_files.append(filepath.name)
                continue

            # æå–å…ƒæ•°æ®
            meta = data.get('_meta', {})

            # æ¸…ç†å·¥ä½œæµæ•°æ®ï¼ˆä¿å­˜åˆ°ä»“åº“çš„ç‰ˆæœ¬ï¼‰
            cleaned_workflow = clean_workflow_for_repo(data)

            # æž„å»ºç´¢å¼•æ¡ç›®
            item = {
                'id': meta.get('id', normalize_id(filepath.name)),
                'name': meta.get('name', 'æœªå‘½å'),
                'description': meta.get('description', ''),
                'author': meta.get('author', 'æœªçŸ¥'),
                'version': meta.get('version', '1.0.0'),
                'vFlowLevel': meta.get('vFlowLevel', 1),
                'homepage': meta.get('homepage', ''),
                'tags': meta.get('tags', []),
                'updated_at': meta.get('updated_at', ''),
                'filename': filepath.name,
                # æž„å»ºä¸‹è½½URL
                'download_url': f"https://raw.githubusercontent.com/ChaoMixian/vFlow-Repos/main/workflows/{filepath.name}",
                # æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºŽè„šæœ¬æ›´æ–°æ–‡ä»¶ï¼‰
                'local_path': str(filepath)
            }

            items.append(item)

            # è‡ªåŠ¨æ›´æ–°æ¸…ç†åŽçš„å·¥ä½œæµæ–‡ä»¶
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(cleaned_workflow, f, ensure_ascii=False, indent=2)

            print(f"âœ… {filepath.name}: {item['name']} (v{item['version']}, Level {item['vFlowLevel']})")

        except json.JSONDecodeError as e:
            errors.append(f"âŒ {filepath.name}: JSONè§£æžé”™è¯¯ - {str(e)}")
            skipped_files.append(filepath.name)
        except Exception as e:
            errors.append(f"âŒ {filepath.name}: {str(e)}")
            skipped_files.append(filepath.name)

    return items, errors, skipped_files


def generate_index(directory='workflows', output_file='index.json'):
    """ç”Ÿæˆç´¢å¼•æ–‡ä»¶"""
    print(f"ðŸ” æ‰«æç›®å½•: {directory}")
    print("=" * 60)

    # æ‰«æå·¥ä½œæµ
    items, errors, skipped_files = scan_directory(directory)

    # æ‰“å°é”™è¯¯å’Œè·³è¿‡çš„æ–‡ä»¶
    if errors:
        print("\nâŒ éªŒè¯å¤±è´¥:")
        for error in errors:
            print(f"  {error}")

    if skipped_files:
        print(f"\nâš ï¸  è·³è¿‡ {len(skipped_files)} ä¸ªæ–‡ä»¶")

    # æŒ‰IDæŽ’åº
    items.sort(key=lambda x: x['id'])

    # æž„å»ºç´¢å¼•
    index = {
        'version': '1.0',
        'last_updated': datetime.now().isoformat(),
        'total_count': len(items),
        'workflows': items
    }

    # å†™å…¥ç´¢å¼•æ–‡ä»¶
    output_path = Path(directory) / output_file
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(index, f, ensure_ascii=False, indent=2)

    print("\n" + "=" * 60)
    print(f"âœ… æˆåŠŸç´¢å¼• {len(items)} ä¸ªå·¥ä½œæµ")
    print(f"ðŸ“ ç´¢å¼•æ–‡ä»¶: {output_path}")
    print(f"ðŸ• æ›´æ–°æ—¶é—´: {index['last_updated']}")

    # å¦‚æžœæœ‰é”™è¯¯ï¼Œè¿”å›žéžé›¶é€€å‡ºç 
    if errors:
        print(f"\nâš ï¸  å­˜åœ¨ {len(errors)} ä¸ªé”™è¯¯ï¼Œè¯·æ£€æŸ¥ï¼")
        sys.exit(1)


def main():
    """ä¸»å‡½æ•°"""
    # é»˜è®¤æ‰«æworkflowsç›®å½•
    workflows_dir = 'workflows'

    # å¦‚æžœæä¾›äº†å‘½ä»¤è¡Œå‚æ•°ï¼Œä½¿ç”¨æŒ‡å®šçš„ç›®å½•
    if len(sys.argv) > 1:
        workflows_dir = sys.argv[1]

    generate_index(workflows_dir)


if __name__ == '__main__':
    main()